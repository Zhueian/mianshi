
    1。分库了，有多个数据源问题
    2。新说法：垂直拆分是分库，拆成多个独立数据库实例（不是一个连接创建三个库，这样性能没提升）eg：order_db 、payment_db 、refund_db
        单体库里的表拆出去别的独立库里。
    3。其实提高性能还是要买机器卖资源。现在都有中间件、框架 性能不会差太多，
        通过调参数tomcat从200qps提升到210 dubbo连接数从500 超时5秒提升到 510 超时6秒。
        意义不大，非得纠结这种为什么选择用java呢，去用c开发啊。
    4。水平分表的两种主流方式，client 和 proxy。
        client是java应用做分库的dbsource路由和分页sql的分表改写，算法逻辑可能是个jar包，性能较好。
        proxy是多了一层转发，也是个服务，且实现了mysql协议，java应用把proxy应用看作一个普通单db，性能差些了。

    5。nginx这种支持几十万长连接的，相比mysql连接数才50多的，远远不是一个量级，所以瓶颈还是在db
    6。单体数据库升级到读写分离，一主多从模式。
        问题一：从库可以用myisam且隔离界别放开为 'ru' 读未提交。
        问题二：有些查询要走主库，怕binlog append顺序id同步延迟。可以用缓存+超时时间来路由：走主库还是从库（如果数据库老是抖）
        代码层：多数据源的mybatisconfig 自定义配置类 和 引入中间件做读写sql转发。
        话术：翼支付账单用的就是旧版本用的就是mycat读写分离分库，分表按月做分表。存量数据有132亿。一主多从，从的数据有存量数据，主是append过去的。
            后来用tidb，java应用层感知为一个单库，所有都让tidbproxy包了。
    7。分表算法：
        按时间：归档数据比较适合这么分表，查询场景是最近的。查询条件要是没有时间，可能要遍历表。
        按范围：根据主键自增雪花id分表。1-100w ，100w到200w一张表。，缺陷同上。
        按取模：用的表较多。orderNo 取模8，就分8个表，但是表固定了。
                以后要扩表到16，可能要走两次寻表算法，历史的取模8和现在取模16 或者 扩表后数据迁移。
        话术：翼支付用的是电信手机号做分表，具体我也不知道，大概是按位数上的数字，11个数字的内在规则分的，这样分的表表散，不至于有的表巨大有的表巨小。
            也做了垂直拆分的多数据源。用户身份信息一个库、用户vip信息一个库、用户资金类信息一个库，都用productNo ConcastNo这些字段。

    8。分库分表后的多字段'查询条'件怎么满足，orderNo、userId 、requestDate等等。
        映射表：用了orderNo做key分表，但是查询条件用userId，要做哥orderNo:userId 的k-v表（nosql也行 mysql也行。）
        二级索引：用es manago构建索引，因为es速度可以 可以构建大量数据的索引（支持分片），查询条件的字段做索引
                监听binlog -> es
                 {
                  'orderNo':'202108231008322222',
                  'userId':'13564081893'，
                  'requestDate':'2021-08-23'
                 }
                缺点：这个有延时，接受不了就做 双写一致 逻辑。
                     分页不好弄排序。
        融合方式：orderNo融入userId和requestDate信息，但是不可取，网监不通过的。
    9。用canal监听mysql的binlog